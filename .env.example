# OpenAI API key. Required for any real run (LLM calls). Example: sk-...
OPENAI_API_KEY=...

# Default OpenAI model used by CQ generator and ontology adapter when not overridden.
# Example: gpt-4o-mini
OPENAI_MODEL=gpt-4o-mini
# CQ generator service URL used by /validate when "generated" column is missing.
# Example: http://127.0.0.1:8001/newapi
EXTERNAL_CQ_GENERATION_URL=http://127.0.0.1:8001/newapi
# Ontology adapter URL used by /ontology/run.
# Example: http://127.0.0.1:8020/generate_ontology
EXTERNAL_ONTOLOGY_SERVICE_URL=http://127.0.0.1:8020/generate_ontology
# Default ontology system for the adapter. One of: ontogenia|domain-ontogen|neon-gpt
ONTOLOGY_SYSTEM=ontogenia

# OOPS pitfall detection
# OOPS API URL. If empty, the OOPS metric is skipped.
# Example: https://oops.linkeddata.es/api/1.2/assess
OOPS_API_URL="https://oops.linkeddata.es/api/1.2/assess"
# OOPS mode: text|file|url
OOPS_API_MODE=text
# OOPS request timeout in seconds.
OOPS_API_TIMEOUT=60

# Output paths (optional overrides)
# Base outputs directory (relative to repo root or absolute).
OUTPUTS_DIR=restapi/outputs
# CQ outputs directory.
CQ_OUTPUTS_DIR=restapi/outputs/cq_validation
# Heatmap output directory.
HEATMAP_OUTPUT_FOLDER=restapi/outputs/cq_validation/heatmaps
# CQ results CSV output directory.
RESULTS_DIR=restapi/outputs/cq_validation/results
# Ontology dataset directory (normalized JSONL files).
ONTOLOGY_DATASET_DIR=datasets/ontology_generation/normalized
# Ontology run output directory.
ONTOLOGY_RUNS_DIR=restapi/outputs/ontology_benchmark/runs

# Ontology benchmark config (optional)
# Timeout for external ontology generation call (seconds).
ONTOLOGY_EXTERNAL_TIMEOUT=300
# Model for OE-Assist LLM evaluation (defaults to OPENAI_MODEL).
ONTOLOGY_LLM_EVAL_MODEL=gpt-4o-mini
# Prompt file path for OE-Assist evaluation.
ONTOLOGY_LLM_EVAL_PROMPT_PATH=datasets/ontology_generation/prompts/oe_assist_prompt.txt
# Max tokens for OE-Assist LLM evaluation.
ONTOLOGY_LLM_EVAL_MAX_TOKENS=800
# Max characters of ontology text passed to the evaluator.
ONTOLOGY_LLM_EVAL_MAX_CHARS=12000

# Ontology adapter OpenAI settings (optional)
# Sampling temperature for ontology generation.
OPENAI_TEMPERATURE=0.2
# Max tokens for ontology generation.
OPENAI_MAX_TOKENS=2000
# Optional system message prepended to ontology prompts (leave empty for none).
OPENAI_SYSTEM_MESSAGE=
# OpenAI client timeout in seconds.
ONTOLOGY_OPENAI_TIMEOUT=60
# Enable HTTP/2 for OpenAI client (true/false).
ONTOLOGY_OPENAI_HTTP2=false
# Disable keepalive connections (true/false).
ONTOLOGY_OPENAI_DISABLE_KEEPALIVE=false
# Max retries for transient OpenAI errors.
ONTOLOGY_OPENAI_MAX_RETRIES=2
# Base retry delay in seconds.
ONTOLOGY_OPENAI_RETRY_BASE_DELAY=1.0
# Max retry delay in seconds.
ONTOLOGY_OPENAI_RETRY_MAX_DELAY=10.0
# Append constraints block to ontology prompts (true/false).
ONTOLOGY_APPEND_CONSTRAINTS=false
# Override prompt template file for ontology adapter (absolute or repo-relative).
ONTOLOGY_PROMPT_FILE=
# Logging level for ontology adapter (DEBUG|INFO|WARNING|ERROR).
LOG_LEVEL=INFO

# UI / CQ validation (optional)
# Bench4KE API base URL used by the UI.
CQ_API_URL=http://127.0.0.1:8000
# UI timeout in seconds (0 disables read timeout).
CQ_API_TIMEOUT=3600
# Flask secret key for sessions (set a random string in production).
FLASK_SECRET_KEY=changeme
# Optional API key forwarded to external CQ generator.
EXTERNAL_CQ_API_KEY=

# Optional providers for CQ generator

TOGETHER_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
# Anthropic API key (only if using Claude provider).
ANTHROPIC_API_KEY=
# Anthropic model name.
ANTHROPIC_MODEL=claude-3-5-sonnet-20240620
